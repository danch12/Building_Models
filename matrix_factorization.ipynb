{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T13:21:58.242463Z",
     "start_time": "2020-01-16T13:21:56.663553Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Matrix Factorization works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think of a factorisation with numbers we may say that we can factor 15 into 3 and 5 Matrix factorisation is quite similar but instead of finding two numbers that multiply together, we are finding two matrices that multiply together. Recommendation systems love using matrix factorisation as by factoring the user x item matrix into a user x feature matrix and a item x feature matrix we uncover underlying features that connect the user and the item. This may make more sense in an example, take for instance Netflix recommending movies, underlying features may be things like if it is a comedy or if it has Will Smith in it. Likewise, the users may have a preference for comedy of a preference for Will Smith. Therefore, if we can break down the items into features and breakdown our users into how much they like those features, we can recommend items with similar features to those that the user likes. For example, if we know that a user likes Will Smith and comedies from his past ratings we can recommend the Fresh Prince Of Bel Air."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have both the item feature matrix and the user feature matrix we can just take the dot product of the individual user and the individual item to get a predicted rating. Below is an simple illustration of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![factorization](https://i.ytimg.com/vi/ZspR5PZemcs/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to make quite a basic Matrix Factorising class myself to try understand better what is going on behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T13:13:27.399500Z",
     "start_time": "2020-01-15T13:13:27.394574Z"
    }
   },
   "outputs": [],
   "source": [
    "#example user - item matrix\n",
    "ratings = np.array([\n",
    "    [2, 5, 3, 0],\n",
    "    [5, 0, 2, 1],\n",
    "    [3, 0, 1, 2],\n",
    "    [3, 3, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T13:13:28.177355Z",
     "start_time": "2020-01-15T13:13:28.145088Z"
    }
   },
   "outputs": [],
   "source": [
    "class rec_sys():\n",
    "    def __init__(self, data, learning_rate=0.01, regularization=0.1, n_factors=10, runs=10):\n",
    "        '''Learn vectors of people and items, \n",
    "        data is dataset containing ratings\n",
    "           factors are how many features you want to include'''\n",
    "    \n",
    "        self.data = data\n",
    "        self.num_users = data.shape[0]\n",
    "        self.num_items = data.shape[1]\n",
    "        #how fast the gradient descent is\n",
    "        #- cant have it too high otherwise you will swing around optimum\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_factors = n_factors\n",
    "        #how many times we run through the data\n",
    "        self.runs = runs\n",
    "        #regularization so that we avoid overfitting\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def fit(self):\n",
    "        '''Learn vectors of people and items, \n",
    "        data is dataset containing ratings\n",
    "           factors are how many features you want to include-\n",
    "           sort of acts like regularisation'''\n",
    "    \n",
    "\n",
    "        #init user and item factors\n",
    "        self.p = np.random.normal(0, 0.1, (self.num_users, self.n_factors))\n",
    "        self.q = np.random.normal(0, 0.1, (self.num_items, self.n_factors))\n",
    "        \n",
    "        #init user and item biases\n",
    "        self.user_bias = np.zeros(self.num_users)\n",
    "        self.item_bias = np.zeros(self.num_items)\n",
    "\n",
    "        self.samples = []\n",
    "        for user in range(self.num_users):\n",
    "            for item in range(self.num_items):\n",
    "                #0s mean that we havent rated yet so dont want to include\n",
    "                if self.data[user, item] > 0:\n",
    "                    self.samples.append((user, item, self.data[user, item]))\n",
    "        \n",
    "\n",
    "        #Gradient Descent\n",
    "        for n in range(self.runs):\n",
    "            for user, item, rating in self.samples:\n",
    "                #comparing actual rating to predicted\n",
    "                prediction = self.predict_rating(user,item)\n",
    "                #getting error\n",
    "                err = rating - prediction\n",
    "                \n",
    "                #adjusting biases based on error\n",
    "                self.user_bias[user] += self.learning_rate * (\n",
    "                    err - self.regularization * self.user_bias[user])\n",
    "                self.item_bias[item] += self.learning_rate * (\n",
    "                    err - self.regularization * self.item_bias[item])\n",
    "\n",
    "                #updating item-feature relationship and user-feature relationship based on error\n",
    "\n",
    "                prev_p = self.p[user,:]\n",
    "                self.p[user,:] += self.learning_rate * (err * self.q[item,:] - self.regularization *self.p[user,:])\n",
    "                self.q[item,:] += self.learning_rate * (err * prev_p - self.regularization * self.q[item,:])\n",
    "     \n",
    "      \n",
    "\n",
    "    def predict_rating(self, user, item):\n",
    "        #0s we havent rated so dont want to include\n",
    "        mean = np.mean(self.data[np.where(self.data != 0)])\n",
    "        prediction = mean + self.user_bias[user] + self.item_bias[item]+ np.dot(self.p[user, :], self.q[item, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T13:13:29.559717Z",
     "start_time": "2020-01-15T13:13:28.955391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing rating item 0 user 0 : 2.1574050664099715\n",
      "new rating item 0 user 5 : 3.2724078324264783\n"
     ]
    }
   ],
   "source": [
    "# creating instance of the system\n",
    "reccomend =rec_sys(data=ratings,learning_rate=0.01,regularization=0.1, n_factors=3,runs=1000)\n",
    "#fitting system\n",
    "reccomend.fit()\n",
    "#predicting existing rating\n",
    "print(f'existing rating item 0 user 0 : {reccomend.predict_rating(0,0)}')\n",
    "#predicting new rating\n",
    "print(f'new rating item 0 user 5 : {reccomend.predict_rating(-1,0)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
